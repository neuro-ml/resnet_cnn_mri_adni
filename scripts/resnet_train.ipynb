{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import lasagne\n",
    "import theano\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers.dnn import Conv3DDNNLayer\n",
    "from lasagne.layers.dnn import Pool3DDNNLayer\n",
    "from lasagne.layers import BatchNormLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.nonlinearities import identity, softmax\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch iteration functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, iterate_minibatches_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor5(name='input', dtype='float32')\n",
    "target_var = T.ivector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    \"\"\"Method for VoxResNet Building.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "        Network dictionary.\n",
    "    \"\"\"\n",
    "    net = {}\n",
    "    net['input'] = InputLayer((None, 1, 110, 110, 110), input_var=input_var)\n",
    "    net['conv1a'] = Conv3DDNNLayer(net['input'], 32, 3, pad='same',\n",
    "                                   nonlinearity=identity)\n",
    "    net['bn1a'] = BatchNormLayer(net['conv1a'])\n",
    "    net['relu1a'] = NonlinearityLayer(net['bn1a'])\n",
    "    net['conv1b'] = Conv3DDNNLayer(net['relu1a'], 32, 3, pad='same',\n",
    "                                   nonlinearity=identity)\n",
    "    net['bn1b'] = BatchNormLayer(net['conv1b'])\n",
    "    net['relu1b'] = NonlinearityLayer(net['bn1b'])\n",
    "    net['conv1c'] = Conv3DDNNLayer(net['relu1b'], 64, 3, stride=(2, 2, 2),\n",
    "                                   pad='same', nonlinearity=identity)\n",
    "    # VoxRes block 2\n",
    "    net['voxres2_bn1'] = BatchNormLayer(net['conv1c'])\n",
    "    net['voxres2_relu1'] = NonlinearityLayer(net['voxres2_bn1'])\n",
    "    net['voxres2_conv1'] = Conv3DDNNLayer(net['voxres2_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres2_bn2'] = BatchNormLayer(net['voxres2_conv1'])\n",
    "    net['voxres2_relu2'] = NonlinearityLayer(net['voxres2_bn2'])\n",
    "    net['voxres2_conv2'] = Conv3DDNNLayer(net['voxres2_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres2_out'] = ElemwiseSumLayer([net['conv1c'],\n",
    "                                           net['voxres2_conv2']])\n",
    "    # VoxRes block 3\n",
    "    net['voxres3_bn1'] = BatchNormLayer(net['voxres2_out'])\n",
    "    net['voxres3_relu1'] = NonlinearityLayer(net['voxres3_bn1'])\n",
    "    net['voxres3_conv1'] = Conv3DDNNLayer(net['voxres3_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres3_bn2'] = BatchNormLayer(net['voxres3_conv1'])\n",
    "    net['voxres3_relu2'] = NonlinearityLayer(net['voxres3_bn2'])\n",
    "    net['voxres3_conv2'] = Conv3DDNNLayer(net['voxres3_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres3_out'] = ElemwiseSumLayer([net['voxres2_out'],\n",
    "                                           net['voxres3_conv2']])\n",
    "\n",
    "    net['bn4'] = BatchNormLayer(net['voxres3_out'])\n",
    "    net['relu4'] = NonlinearityLayer(net['bn4'])\n",
    "    net['conv4'] = Conv3DDNNLayer(net['relu4'], 64, 3, stride=(2, 2, 2),\n",
    "                                  pad='same', nonlinearity=identity)\n",
    "    # VoxRes block 5\n",
    "    net['voxres5_bn1'] = BatchNormLayer(net['conv4'])\n",
    "    net['voxres5_relu1'] = NonlinearityLayer(net['voxres5_bn1'])\n",
    "    net['voxres5_conv1'] = Conv3DDNNLayer(net['voxres5_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres5_bn2'] = BatchNormLayer(net['voxres5_conv1'])\n",
    "    net['voxres5_relu2'] = NonlinearityLayer(net['voxres5_bn2'])\n",
    "    net['voxres5_conv2'] = Conv3DDNNLayer(net['voxres5_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres5_out'] = ElemwiseSumLayer([net['conv4'], net['voxres5_conv2']])\n",
    "    # VoxRes block 6\n",
    "    net['voxres6_bn1'] = BatchNormLayer(net['voxres5_out'])\n",
    "    net['voxres6_relu1'] = NonlinearityLayer(net['voxres6_bn1'])\n",
    "    net['voxres6_conv1'] = Conv3DDNNLayer(net['voxres6_relu1'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres6_bn2'] = BatchNormLayer(net['voxres6_conv1'])\n",
    "    net['voxres6_relu2'] = NonlinearityLayer(net['voxres6_bn2'])\n",
    "    net['voxres6_conv2'] = Conv3DDNNLayer(net['voxres6_relu2'], 64, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres6_out'] = ElemwiseSumLayer([net['voxres5_out'],\n",
    "                                           net['voxres6_conv2']])\n",
    "\n",
    "    net['bn7'] = BatchNormLayer(net['voxres6_out'])\n",
    "    net['relu7'] = NonlinearityLayer(net['bn7'])\n",
    "    net['conv7'] = Conv3DDNNLayer(net['relu7'], 128, 3, stride=(2, 2, 2),\n",
    "                                  pad='same', nonlinearity=identity)\n",
    "\n",
    "    # VoxRes block 8\n",
    "    net['voxres8_bn1'] = BatchNormLayer(net['conv7'])\n",
    "    net['voxres8_relu1'] = NonlinearityLayer(net['voxres8_bn1'])\n",
    "    net['voxres8_conv1'] = Conv3DDNNLayer(net['voxres8_relu1'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres8_bn2'] = BatchNormLayer(net['voxres8_conv1'])\n",
    "    net['voxres8_relu2'] = NonlinearityLayer(net['voxres8_bn2'])\n",
    "    net['voxres8_conv2'] = Conv3DDNNLayer(net['voxres8_relu2'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres8_out'] = ElemwiseSumLayer([net['conv7'], net['voxres8_conv2']])\n",
    "    # VoxRes block 9\n",
    "    net['voxres9_bn1'] = BatchNormLayer(net['voxres8_out'])\n",
    "    net['voxres9_relu1'] = NonlinearityLayer(net['voxres9_bn1'])\n",
    "    net['voxres9_conv1'] = Conv3DDNNLayer(net['voxres9_relu1'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres9_bn2'] = BatchNormLayer(net['voxres9_conv1'])\n",
    "    net['voxres9_relu2'] = NonlinearityLayer(net['voxres9_bn2'])\n",
    "    net['voxres9_conv2'] = Conv3DDNNLayer(net['voxres9_relu2'], 128, 3,\n",
    "                                          pad='same', nonlinearity=identity)\n",
    "    net['voxres9_out'] = ElemwiseSumLayer([net['voxres8_out'],\n",
    "                                           net['voxres9_conv2']])\n",
    "\n",
    "    net['pool10'] = Pool3DDNNLayer(net['voxres9_out'], 7)\n",
    "    net['fc11'] = DenseLayer(net['pool10'], 128)\n",
    "    net['prob'] = DenseLayer(net['fc11'], 2, nonlinearity=softmax)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(format='[%(asctime)s]  %(message)s',\n",
    "                    datefmt='%d.%m %H:%M:%S',\n",
    "                    level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(first_class, second_class, results_folder,\n",
    "                 num_epochs=70, batchsize=3):\n",
    "    \"\"\"Iterate minibatches on train subset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_class : {'AD', 'LMCI', 'EMCI', 'Normal'}\n",
    "        String label for target == 0.\n",
    "    second_class : {'AD', 'LMCI', 'EMCI', 'Normal'}\n",
    "        String label for target == 1.\n",
    "    results_folder : string\n",
    "        Folder to store results.\n",
    "    num_epochs : integer\n",
    "        Number of epochs for all of the experiments. Default is 70.\n",
    "    batchsize : integer\n",
    "        Batchsize for network training. Default is 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    if first_class not in {'AD', 'LMCI', 'EMCI', 'Normal'}:\n",
    "        msg = \"First class must be 'AD', 'LMCI', 'EMCI' or 'Normal', not {0}\"\n",
    "        raise ValueError(msg.format(first_class))\n",
    "    \n",
    "    if second_class not in {'AD', 'LMCI', 'EMCI', 'Normal'}:\n",
    "        msg = \"Second class must be 'AD', 'LMCI', 'EMCI' or 'Normal', not {0}\"\n",
    "        raise ValueError(msg.format(second_class))\n",
    "        \n",
    "    if first_class == second_class:\n",
    "        raise ValueError(\"Class labels should be different\")\n",
    "        \n",
    "    if not os.path.exists(results_folder):\n",
    "        os.makedirs(results_folder)\n",
    "    \n",
    "    metadata = pd.read_csv('data/metadata.csv')\n",
    "    smc_mask = ((metadata.Label == first_class) |\n",
    "                (metadata.Label == second_class)).values.astype('bool')\n",
    "    data = np.zeros((smc_mask.sum(), 1, 110, 110, 110), dtype='float32')\n",
    "\n",
    "    for it, im in tqdm(enumerate(metadata[smc_mask].Path.values),\n",
    "                       total=smc_mask.sum(), desc='Reading MRI to memory'):\n",
    "        mx = nib.load(im).get_data().max(axis=0).max(axis=0).max(axis=0)\n",
    "        data[it, 0, :, :, :] = np.array(nib.load(im).get_data()) / mx\n",
    "\n",
    "    target = (metadata[smc_mask].Label == second_class).values.astype('int32')\n",
    "    \n",
    "    for cvrand in range(5):\n",
    "        cv = StratifiedKFold(target, n_folds=5, random_state=42 * cvrand,\n",
    "                             shuffle=True)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(cv):\n",
    "            logging.debug('Starting fold {}'.format(fold))\n",
    "            X_train, y_train = data[train_index], target[train_index]\n",
    "            X_test, y_test = data[test_index], target[test_index]\n",
    "\n",
    "            net = build_net()\n",
    "\n",
    "            prediction = lasagne.layers.get_output(net['prob'])\n",
    "            loss = lasagne.objectives.categorical_crossentropy(prediction,\n",
    "                                                               target_var)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            params = lasagne.layers.get_all_params(net['prob'], trainable=True)\n",
    "            updates = lasagne.updates.nesterov_momentum(loss, params, 0.001)\n",
    "\n",
    "            test_prediction = lasagne.layers.get_output(net['prob'],\n",
    "                                                        deterministic=True)\n",
    "            test_loss = categorical_crossentropy(test_prediction, target_var)\n",
    "            test_loss = test_loss.mean()\n",
    "\n",
    "            train_fn = theano.function([input_var, target_var], loss,\n",
    "                                       updates=updates)\n",
    "            val_fn = theano.function([input_var, target_var], test_loss)\n",
    "            test_fn = theano.function([input_var], test_prediction)\n",
    "\n",
    "            logging.debug(\"Done building net\")\n",
    "\n",
    "            eps = []\n",
    "            tr_losses = []\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            val_rocs = []\n",
    "\n",
    "            logging.debug(\"Starting training...\")\n",
    "            den = X_train.shape[0] / batchsize\n",
    "            for epoch in range(num_epochs):\n",
    "                train_err = 0\n",
    "                train_batches = 0\n",
    "                start_time = time.time()\n",
    "                t = tqdm(iterate_minibatches_train(X_train, y_train, batchsize,\n",
    "                                                   shuffle=True),\n",
    "                         total=int(den),\n",
    "                         desc='Epoch {}, Loss = inf'.format(epoch + 1))\n",
    "                for batch in t:\n",
    "                    inputs, targets = batch\n",
    "                    train_err += train_fn(inputs, targets)\n",
    "                    train_batches += 1\n",
    "                    ls = train_err / train_batches\n",
    "                    t.set_description('Epoch {}, Loss = {:.5f}'.format(epoch +\n",
    "                                                                       1, ls))\n",
    "\n",
    "                val_err = 0\n",
    "                val_batches = 0\n",
    "                preds = []\n",
    "                targ = []\n",
    "                for batch in iterate_minibatches(X_test, y_test, batchsize,\n",
    "                                                 shuffle=False):\n",
    "                    inputs, targets = batch\n",
    "                    err = val_fn(inputs, targets)\n",
    "                    val_err += err\n",
    "                    val_batches += 1\n",
    "                    out = test_fn(inputs)\n",
    "                    [preds.append(i) for i in out]\n",
    "                    [targ.append(i) for i in targets]\n",
    "\n",
    "                logging.debug(\"Epoch {} done\".format(epoch + 1))\n",
    "                print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                    epoch + 1, num_epochs, time.time() - start_time),\n",
    "                    flush=True)\n",
    "                print(\"  training loss:\\t\\t{:.7f}\".format(\n",
    "                    train_err / train_batches), flush=True)\n",
    "                print(\"  validation loss:\\t\\t{:.7f}\".format(\n",
    "                    val_err / val_batches), flush=True)\n",
    "                print(\"  validation accuracy:\\t\\t{:.7f}\".format(\n",
    "                    accuracy_score(np.array(targ),\n",
    "                                   np.array(preds).argmax(axis=1))),\n",
    "                      flush=True)\n",
    "                print(\"  validation auc:\\t\\t{:.7f}\".format(\n",
    "                    roc_auc_score(np.array(targ),\n",
    "                                  np.array(preds)[:, 1])), flush=True)\n",
    "\n",
    "                eps.append(epoch)\n",
    "                tr_losses.append(train_err / train_batches)\n",
    "                val_losses.append(val_err / val_batches)\n",
    "                val_accs.append(accuracy_score(np.array(targ),\n",
    "                                               np.array(preds).argmax(axis=1)))\n",
    "                val_rocs.append(roc_auc_score(np.array(targ),\n",
    "                                              np.array(preds)[:, 1]))\n",
    "\n",
    "\n",
    "            np.save('./{}/{}_{}nm_eps.npy'.format(results_folder, \n",
    "                                                  cvrand, fold),\n",
    "                    np.array(eps))\n",
    "            np.save('./{}/{}_{}nm_tr_loss.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(tr_losses))\n",
    "            np.save('./{}/{}_{}nm_vl_loss.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_losses))\n",
    "            np.save('./{}/{}_{}nm_vl_accs.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_accs))\n",
    "            np.save('./{}/{}_{}nm_vl_rocs.npy'.format(results_folder, \n",
    "                                                      cvrand, fold),\n",
    "                    np.array(val_rocs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_training('AD', 'Normal', './results_resnet/ad_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('AD', 'EMCI', './results_resnet/ad_vs_emci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('AD', 'LMCI', './results_resnet/ad_vs_lmci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('EMCI', 'Normal', './results_resnet/emci_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('LMCI', 'Normal', './results_resnet/lmci_vs_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training('LMCI', 'EMCI', './results_resnet/lmci_vs_emci')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
